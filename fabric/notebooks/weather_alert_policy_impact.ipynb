{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0000001",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# WEATHER ALERT POLICY IMPACT MONITOR\n",
    "# Parametric Insurance Demo\n",
    "# ============================================================================\n",
    "#\n",
    "#   Step 0: Configuration & Imports\n",
    "#   Step 1: Poll NOAA for Active Severe Weather Alerts (all US)\n",
    "#   Step 2: Load Active Policies from Lakehouse\n",
    "#   Step 3: Geo-match alerts to policyholders (lat/lon bounding box)\n",
    "#   Step 4: Deduplicate against previously processed alerts\n",
    "#   Step 5: Publish policy.weather.impact events to Event Grid\n",
    "#   Step 6: Persist impact records to Delta table + summary dashboard\n",
    "#\n",
    "# Prerequisites:\n",
    "#   - schema_load_with_policies.ipynb must have been run first\n",
    "#   - parametric_insurance_unified_demo_new.ipynb (optional ‚Äî enriches claims context)\n",
    "#\n",
    "# Event Grid event published:\n",
    "#   Type:    policy.weather.impact\n",
    "#   Subject: weather/alert/<alert_id>/policy/<policy_id>\n",
    "#   Data:    { alert details, policy details, impact assessment }\n",
    "# ============================================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-0001",
   "metadata": {},
   "source": [
    "# üå©Ô∏è Weather Alert Policy Impact Monitor\n",
    "\n",
    "| Step | Description | Source | Event Grid |\n",
    "|------|-------------|--------|------------|\n",
    "| 1 | Poll severe weather alerts | **NOAA API** (free) | ‚Äî |\n",
    "| 2 | Load active policies | **Fabric Lakehouse** | ‚Äî |\n",
    "| 3 | Geo-match (lat/lon radius) | Spark computation | ‚Äî |\n",
    "| 4 | Deduplicate alerts | Delta table lookup | ‚Äî |\n",
    "| 5 | Publish impact events | ‚Äî | `policy.weather.impact` |\n",
    "| 6 | Save + dashboard | Delta tables | ‚Äî |\n",
    "\n",
    "> ‚ö° Run this notebook on a schedule (e.g. every 15 minutes) or trigger it manually for a demo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-0002",
   "metadata": {},
   "source": [
    "---\n",
    "## üîß Step 0 ‚Äî Configuration & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0000002",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "%pip install azure-core --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0000003",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import uuid\n",
    "import math\n",
    "import requests\n",
    "import warnings\n",
    "import notebookutils\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, StringType, IntegerType,\n",
    "    DoubleType, TimestampType, BooleanType, FloatType\n",
    ")\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# CONFIGURATION ‚Äî Edit these values for your environment\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "@dataclass\n",
    "class AlertMonitorConfig:\n",
    "    \"\"\"Configuration for the weather alert policy impact monitor.\"\"\"\n",
    "\n",
    "    # -- NOAA API --\n",
    "    noaa_api_url: str = \"https://api.weather.gov\"\n",
    "    noaa_user_agent: str = \"ParametricInsuranceDemo/1.0 (kalateef@microsoft.com)\"\n",
    "\n",
    "    # Alert severity filter ‚Äî only process alerts at or above this severity.\n",
    "    # NOAA severity levels (ascending): Unknown, Minor, Moderate, Severe, Extreme\n",
    "    min_severity: str = \"Severe\"  # Moderate | Severe | Extreme\n",
    "\n",
    "    # Alert urgency filter ‚Äî Immediate | Expected | Future | Past | Unknown\n",
    "    min_urgency: str = \"Expected\"\n",
    "\n",
    "    # Geographic match radius in kilometres around each policy's lat/lon.\n",
    "    # NOAA alert polygons can cover large areas; 50 km is a reasonable proxy\n",
    "    # for \"meaningfully close\" without being overly broad.\n",
    "    alert_radius_km: float = 50.0\n",
    "\n",
    "    # Deduplication window: don't re-raise an impact for the same\n",
    "    # (alert_id, policy_id) pair within this many hours.\n",
    "    dedup_window_hours: int = 6\n",
    "\n",
    "    # -- Azure Event Grid (optional ‚Äî leave blank for local-only mode) --\n",
    "    eventgrid_topic_endpoint: str = \"<eventgrid-endpoint>\"\n",
    "    eventgrid_topic_key: str = \"<eventgrid-key>\"\n",
    "\n",
    "    # Event type published to Event Grid\n",
    "    EVT_POLICY_WEATHER_IMPACT: str = \"policy.weather.impact\"\n",
    "\n",
    "\n",
    "config = AlertMonitorConfig()\n",
    "\n",
    "# Override from Variable Library or environment\n",
    "try:\n",
    "    env_lib = notebookutils.variableLibrary.getLibrary(\"environmentVariables\")\n",
    "    config.eventgrid_topic_endpoint = getattr(env_lib, \"EVENTGRID_TOPIC_ENDPOINT\", config.eventgrid_topic_endpoint)\n",
    "    config.eventgrid_topic_key      = getattr(env_lib, \"EVENTGRID_TOPIC_KEY\",      config.eventgrid_topic_key)\n",
    "    config.noaa_user_agent          = getattr(env_lib, \"NOAA_USER_AGENT\",          config.noaa_user_agent)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Notebook widget overrides (for pipeline parameterisation)\n",
    "try:\n",
    "    config.eventgrid_topic_endpoint = config.eventgrid_topic_endpoint or notebookutils.mssparkutils.widgets.get(\"eventgrid_endpoint\")\n",
    "    config.eventgrid_topic_key      = config.eventgrid_topic_key      or notebookutils.mssparkutils.widgets.get(\"eventgrid_key\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "EVENTGRID_ENABLED = bool(\n",
    "    config.eventgrid_topic_endpoint and\n",
    "    config.eventgrid_topic_key and\n",
    "    config.eventgrid_topic_endpoint != \"<eventgrid-endpoint>\"\n",
    ")\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "SEVERITY_ORDER = [\"Unknown\", \"Minor\", \"Moderate\", \"Severe\", \"Extreme\"]\n",
    "URGENCY_ORDER  = [\"Unknown\", \"Past\", \"Future\", \"Expected\", \"Immediate\"]\n",
    "\n",
    "RUN_ID  = uuid.uuid4().hex[:8].upper()\n",
    "now_utc = datetime.now(timezone.utc).replace(tzinfo=None)   # naive UTC for Spark\n",
    "\n",
    "print(f\"üå©Ô∏è  Weather Alert Policy Impact Monitor\")\n",
    "print(f\"   Run ID:           {RUN_ID}\")\n",
    "print(f\"   Timestamp (UTC):  {now_utc.isoformat()}Z\")\n",
    "print(f\"   Min severity:     {config.min_severity}\")\n",
    "print(f\"   Match radius:     {config.alert_radius_km} km\")\n",
    "print(f\"   Dedup window:     {config.dedup_window_hours} h\")\n",
    "print(f\"   Event Grid:       {'‚úÖ ENABLED' if EVENTGRID_ENABLED else '‚ö†Ô∏è  DISABLED (local-only mode)'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-0003",
   "metadata": {},
   "source": [
    "---\n",
    "## üì° Event Grid Client + Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0000004",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Lightweight Event Grid publisher ‚Äî no SDK dependency\n",
    "# ============================================================================\n",
    "\n",
    "class NotebookEventGridClient:\n",
    "    \"\"\"Publishes CloudEvents-compatible events via the Event Grid REST API.\"\"\"\n",
    "\n",
    "    def __init__(self, endpoint: str, key: str):\n",
    "        self.endpoint = endpoint.rstrip(\"/\")\n",
    "        self.key = key\n",
    "        self.audit_log: List[Dict[str, Any]] = []\n",
    "        self._counter = 0\n",
    "\n",
    "    def publish_event(\n",
    "        self,\n",
    "        event_type: str,\n",
    "        subject: str,\n",
    "        data: Dict[str, Any],\n",
    "        data_version: str = \"1.0\",\n",
    "    ) -> bool:\n",
    "        event_id   = str(uuid.uuid4())\n",
    "        event_time = datetime.utcnow().isoformat() + \"Z\"\n",
    "        self._counter += 1\n",
    "\n",
    "        payload = [{\n",
    "            \"id\":          event_id,\n",
    "            \"eventType\":   event_type,\n",
    "            \"subject\":     subject,\n",
    "            \"eventTime\":   event_time,\n",
    "            \"dataVersion\": data_version,\n",
    "            \"data\":        data,\n",
    "        }]\n",
    "\n",
    "        log_entry = {\n",
    "            \"seq\":        self._counter,\n",
    "            \"event_id\":   event_id,\n",
    "            \"event_type\": event_type,\n",
    "            \"subject\":    subject,\n",
    "            \"event_time\": event_time,\n",
    "            \"data_summary\": json.dumps({k: v for k, v in data.items() if k in (\n",
    "                \"policy_id\", \"business_name\", \"alert_id\", \"alert_event\",\n",
    "                \"alert_severity\", \"city\", \"state\"\n",
    "            )}),\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            resp = requests.post(\n",
    "                self.endpoint,\n",
    "                headers={\n",
    "                    \"aeg-sas-key\":  self.key,\n",
    "                    \"Content-Type\": \"application/json\",\n",
    "                },\n",
    "                json=payload,\n",
    "                timeout=10,\n",
    "            )\n",
    "            resp.raise_for_status()\n",
    "            log_entry[\"status\"] = \"published\"\n",
    "            log_entry[\"error\"]  = None\n",
    "            self.audit_log.append(log_entry)\n",
    "            return True\n",
    "        except Exception as exc:\n",
    "            log_entry[\"status\"] = \"failed\"\n",
    "            log_entry[\"error\"]  = str(exc)\n",
    "            self.audit_log.append(log_entry)\n",
    "            print(f\"  ‚ö†Ô∏è  Event Grid publish failed: {exc}\")\n",
    "            return False\n",
    "\n",
    "\n",
    "eg_client: Optional[NotebookEventGridClient] = (\n",
    "    NotebookEventGridClient(config.eventgrid_topic_endpoint, config.eventgrid_topic_key)\n",
    "    if EVENTGRID_ENABLED else None\n",
    ")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Geo-distance helper (Haversine formula)\n",
    "# ============================================================================\n",
    "\n",
    "def haversine_km(lat1: float, lon1: float, lat2: float, lon2: float) -> float:\n",
    "    \"\"\"Return the great-circle distance in km between two (lat, lon) points.\"\"\"\n",
    "    R = 6371.0\n",
    "    phi1, phi2 = math.radians(lat1), math.radians(lat2)\n",
    "    dphi   = math.radians(lat2 - lat1)\n",
    "    dlambda = math.radians(lon2 - lon1)\n",
    "    a = math.sin(dphi / 2)**2 + math.cos(phi1) * math.cos(phi2) * math.sin(dlambda / 2)**2\n",
    "    return R * 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
    "\n",
    "\n",
    "def severity_rank(s: str) -> int:\n",
    "    return SEVERITY_ORDER.index(s) if s in SEVERITY_ORDER else 0\n",
    "\n",
    "def urgency_rank(u: str) -> int:\n",
    "    return URGENCY_ORDER.index(u) if u in URGENCY_ORDER else 0\n",
    "\n",
    "\n",
    "print(\"‚úÖ Event Grid client + utility functions ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-0004",
   "metadata": {},
   "source": [
    "---\n",
    "## üåê Step 1 ‚Äî Poll NOAA for Active Severe Weather Alerts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0000005",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "def fetch_noaa_alerts(min_severity: str = \"Moderate\") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Fetch all active NWS alerts for the contiguous US, filtered by\n",
    "    minimum severity and urgency.\n",
    "\n",
    "    Returns a list of normalised alert dicts with a centroid lat/lon\n",
    "    derived from the alert geometry (or affected area geocoding).\n",
    "    \"\"\"\n",
    "    headers = {\"User-Agent\": config.noaa_user_agent, \"Accept\": \"application/geo+json\"}\n",
    "    url     = f\"{config.noaa_api_url}/alerts/active?status=actual&message_type=alert&region_type=land\"\n",
    "\n",
    "    try:\n",
    "        resp = requests.get(url, headers=headers, timeout=20)\n",
    "        resp.raise_for_status()\n",
    "        features = resp.json().get(\"features\", [])\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ö†Ô∏è  NOAA alert fetch error: {e}\")\n",
    "        return []\n",
    "\n",
    "    print(f\"  Raw NOAA features returned: {len(features)}\")\n",
    "\n",
    "    alerts = []\n",
    "    min_sev_rank = severity_rank(min_severity)\n",
    "    min_urg_rank = urgency_rank(config.min_urgency)\n",
    "\n",
    "    for feat in features:\n",
    "        props = feat.get(\"properties\", {})\n",
    "\n",
    "        sev = props.get(\"severity\", \"Unknown\")\n",
    "        urg = props.get(\"urgency\",  \"Unknown\")\n",
    "\n",
    "        if severity_rank(sev) < min_sev_rank:\n",
    "            continue\n",
    "        if urgency_rank(urg) < min_urg_rank:\n",
    "            continue\n",
    "\n",
    "        # Derive a centroid from geometry (Polygon / MultiPolygon)\n",
    "        centroid_lat, centroid_lon = None, None\n",
    "        geometry = feat.get(\"geometry\")\n",
    "        if geometry and geometry.get(\"type\") == \"Polygon\":\n",
    "            coords = geometry[\"coordinates\"][0]  # outer ring\n",
    "            if coords:\n",
    "                centroid_lon = sum(c[0] for c in coords) / len(coords)\n",
    "                centroid_lat = sum(c[1] for c in coords) / len(coords)\n",
    "        elif geometry and geometry.get(\"type\") == \"MultiPolygon\":\n",
    "            all_coords = [c for ring in geometry[\"coordinates\"] for c in ring[0]]\n",
    "            if all_coords:\n",
    "                centroid_lon = sum(c[0] for c in all_coords) / len(all_coords)\n",
    "                centroid_lat = sum(c[1] for c in all_coords) / len(all_coords)\n",
    "\n",
    "        # Affected areas (used when geometry is absent or as enrichment)\n",
    "        affected_zones  = props.get(\"affectedZones\", [])  # list of zone URLs\n",
    "        area_desc       = props.get(\"areaDesc\", \"\")\n",
    "\n",
    "        # Parse onset / expires timestamps\n",
    "        def _ts(s):\n",
    "            if not s:\n",
    "                return None\n",
    "            try:\n",
    "                return datetime.fromisoformat(s.replace(\"Z\", \"+00:00\")).replace(tzinfo=None)\n",
    "            except Exception:\n",
    "                return None\n",
    "\n",
    "        alert = {\n",
    "            \"alert_id\":      props.get(\"id\", feat.get(\"id\", str(uuid.uuid4()))),\n",
    "            \"alert_event\":   props.get(\"event\", \"Unknown\"),\n",
    "            \"headline\":      props.get(\"headline\", \"\"),\n",
    "            \"description\":   props.get(\"description\", \"\")[:1000],  # truncate for storage\n",
    "            \"instruction\":   props.get(\"instruction\", \"\")[:500],\n",
    "            \"severity\":      sev,\n",
    "            \"urgency\":       urg,\n",
    "            \"certainty\":     props.get(\"certainty\", \"Unknown\"),\n",
    "            \"area_desc\":     area_desc,\n",
    "            \"sender_name\":   props.get(\"senderName\", \"\"),\n",
    "            \"onset\":         _ts(props.get(\"onset\")),\n",
    "            \"expires\":       _ts(props.get(\"expires\")),\n",
    "            \"sent\":          _ts(props.get(\"sent\")),\n",
    "            \"centroid_lat\":  centroid_lat,\n",
    "            \"centroid_lon\":  centroid_lon,\n",
    "            \"zone_count\":    len(affected_zones),\n",
    "            \"fetched_at\":    now_utc,\n",
    "        }\n",
    "        alerts.append(alert)\n",
    "\n",
    "    return alerts\n",
    "\n",
    "\n",
    "print(\"üåê Polling NOAA for active severe weather alerts...\")\n",
    "raw_alerts = fetch_noaa_alerts(config.min_severity)\n",
    "\n",
    "# Filter to those with a usable centroid for geo-matching\n",
    "geo_alerts  = [a for a in raw_alerts if a[\"centroid_lat\"] is not None]\n",
    "no_geo      = len(raw_alerts) - len(geo_alerts)\n",
    "\n",
    "print(f\"\\n  Total qualifying alerts: {len(raw_alerts)}\")\n",
    "print(f\"  Alerts with geometry:    {len(geo_alerts)}\")\n",
    "print(f\"  Alerts without geometry: {no_geo} (skipped ‚Äî no centroid)\")\n",
    "print()\n",
    "\n",
    "if geo_alerts:\n",
    "    # Show breakdown by event type\n",
    "    from collections import Counter\n",
    "    event_counts = Counter(a[\"alert_event\"] for a in geo_alerts)\n",
    "    for evt, cnt in event_counts.most_common(10):\n",
    "        print(f\"  {cnt:3d} √ó {evt}\")\n",
    "else:\n",
    "    print(\"  ‚ÑπÔ∏è  No geo-located alerts found at the configured severity level.\")\n",
    "    print(\"      This is normal during calm weather. The notebook will complete gracefully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-0005",
   "metadata": {},
   "source": [
    "---\n",
    "## üè¶ Step 2 ‚Äî Load Active Policies from Lakehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0000006",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "print(\"üè¶ Loading active policies from Lakehouse...\")\n",
    "\n",
    "try:\n",
    "    policies_df = spark.sql(\"\"\"\n",
    "        SELECT\n",
    "            policy_id,\n",
    "            business_name,\n",
    "            business_type,\n",
    "            city,\n",
    "            state,\n",
    "            zip_code,\n",
    "            address,\n",
    "            latitude,\n",
    "            longitude,\n",
    "            threshold_minutes,\n",
    "            hourly_rate,\n",
    "            max_payout,\n",
    "            contact_email,\n",
    "            contact_phone,\n",
    "            status\n",
    "        FROM policies\n",
    "        WHERE status = 'active'\n",
    "          AND latitude  IS NOT NULL\n",
    "          AND longitude IS NOT NULL\n",
    "    \"\"\")\n",
    "    policies = [row.asDict() for row in policies_df.collect()]\n",
    "    print(f\"  ‚úÖ Loaded {len(policies)} active policies.\")\n",
    "except Exception as e:\n",
    "    print(f\"  ‚ùå Could not load policies: {e}\")\n",
    "    print(\"     Ensure schema_load_with_policies.ipynb has been run first.\")\n",
    "    policies = []\n",
    "\n",
    "if policies:\n",
    "    states = sorted(set(p[\"state\"] for p in policies))\n",
    "    print(f\"  States covered: {', '.join(states)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-0006",
   "metadata": {},
   "source": [
    "---\n",
    "## üìç Step 3 ‚Äî Geo-Match Alerts to Policyholders (Lat/Lon Radius)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0000007",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"üìç Geo-matching {len(geo_alerts)} alert(s) against {len(policies)} policies...\")\n",
    "print(f\"   Match radius: {config.alert_radius_km} km\\n\")\n",
    "\n",
    "impacts: List[Dict[str, Any]] = []\n",
    "\n",
    "for alert in geo_alerts:\n",
    "    a_lat = alert[\"centroid_lat\"]\n",
    "    a_lon = alert[\"centroid_lon\"]\n",
    "\n",
    "    for policy in policies:\n",
    "        p_lat = float(policy[\"latitude\"])\n",
    "        p_lon = float(policy[\"longitude\"])\n",
    "\n",
    "        dist_km = haversine_km(a_lat, a_lon, p_lat, p_lon)\n",
    "\n",
    "        if dist_km <= config.alert_radius_km:\n",
    "            # Estimate a risk score based on severity, urgency, and proximity\n",
    "            sev_score  = severity_rank(alert[\"severity\"]) / (len(SEVERITY_ORDER) - 1)\n",
    "            urg_score  = urgency_rank(alert[\"urgency\"])   / (len(URGENCY_ORDER)  - 1)\n",
    "            prox_score = 1.0 - (dist_km / config.alert_radius_km)  # 1 = at centroid, 0 = at boundary\n",
    "            risk_score = round((sev_score * 0.45 + urg_score * 0.35 + prox_score * 0.20), 3)\n",
    "\n",
    "            impact_id = f\"WI-{RUN_ID}-{uuid.uuid4().hex[:6].upper()}\"\n",
    "\n",
    "            impact = {\n",
    "                # IDs\n",
    "                \"impact_id\":        impact_id,\n",
    "                \"alert_id\":         alert[\"alert_id\"],\n",
    "                \"policy_id\":        policy[\"policy_id\"],\n",
    "                \"run_id\":           RUN_ID,\n",
    "                # Alert details\n",
    "                \"alert_event\":      alert[\"alert_event\"],\n",
    "                \"alert_headline\":   alert[\"headline\"],\n",
    "                \"alert_description\":alert[\"description\"],\n",
    "                \"alert_instruction\":alert[\"instruction\"],\n",
    "                \"alert_severity\":   alert[\"severity\"],\n",
    "                \"alert_urgency\":    alert[\"urgency\"],\n",
    "                \"alert_certainty\":  alert[\"certainty\"],\n",
    "                \"alert_area_desc\":  alert[\"area_desc\"],\n",
    "                \"alert_sender\":     alert[\"sender_name\"],\n",
    "                \"alert_onset\":      alert[\"onset\"],\n",
    "                \"alert_expires\":    alert[\"expires\"],\n",
    "                \"alert_centroid_lat\": float(a_lat),\n",
    "                \"alert_centroid_lon\": float(a_lon),\n",
    "                # Policy details\n",
    "                \"business_name\":    policy[\"business_name\"],\n",
    "                \"business_type\":    policy[\"business_type\"],\n",
    "                \"city\":             policy[\"city\"],\n",
    "                \"state\":            policy[\"state\"],\n",
    "                \"zip_code\":         policy[\"zip_code\"],\n",
    "                \"contact_email\":    policy[\"contact_email\"],\n",
    "                \"contact_phone\":    policy[\"contact_phone\"],\n",
    "                \"threshold_minutes\":int(policy[\"threshold_minutes\"]),\n",
    "                \"hourly_rate\":      float(policy[\"hourly_rate\"]),\n",
    "                \"max_payout\":       float(policy[\"max_payout\"]),\n",
    "                # Geo match\n",
    "                \"policy_lat\":       float(p_lat),\n",
    "                \"policy_lon\":       float(p_lon),\n",
    "                \"distance_km\":      round(dist_km, 2),\n",
    "                # Risk\n",
    "                \"risk_score\":       risk_score,\n",
    "                \"impact_status\":    \"pending_notification\",\n",
    "                \"created_at\":       now_utc,\n",
    "            }\n",
    "            impacts.append(impact)\n",
    "\n",
    "print(f\"  Raw geo-matches found: {len(impacts)}\")\n",
    "\n",
    "if not impacts:\n",
    "    print(\"\\n  ‚ÑπÔ∏è  No policyholders fall within the configured radius for the current alerts.\")\n",
    "    print(\"      Consider widening alert_radius_km or lowering min_severity for testing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-0007",
   "metadata": {},
   "source": [
    "---\n",
    "## üîÅ Step 4 ‚Äî Deduplicate Against Previously Processed Alerts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0000008",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Create the weather_impact_events table if it doesn't exist yet\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS weather_impact_events (\n",
    "        impact_id          STRING,\n",
    "        alert_id           STRING,\n",
    "        policy_id          STRING,\n",
    "        run_id             STRING,\n",
    "        alert_event        STRING,\n",
    "        alert_headline     STRING,\n",
    "        alert_description  STRING,\n",
    "        alert_instruction  STRING,\n",
    "        alert_severity     STRING,\n",
    "        alert_urgency      STRING,\n",
    "        alert_certainty    STRING,\n",
    "        alert_area_desc    STRING,\n",
    "        alert_sender       STRING,\n",
    "        alert_onset        TIMESTAMP,\n",
    "        alert_expires      TIMESTAMP,\n",
    "        alert_centroid_lat DOUBLE,\n",
    "        alert_centroid_lon DOUBLE,\n",
    "        business_name      STRING,\n",
    "        business_type      STRING,\n",
    "        city               STRING,\n",
    "        state              STRING,\n",
    "        zip_code           STRING,\n",
    "        contact_email      STRING,\n",
    "        contact_phone      STRING,\n",
    "        threshold_minutes  INT,\n",
    "        hourly_rate        DOUBLE,\n",
    "        max_payout         DOUBLE,\n",
    "        policy_lat         DOUBLE,\n",
    "        policy_lon         DOUBLE,\n",
    "        distance_km        DOUBLE,\n",
    "        risk_score         DOUBLE,\n",
    "        impact_status      STRING,\n",
    "        eventgrid_status   STRING,\n",
    "        created_at         TIMESTAMP\n",
    "    ) USING DELTA\n",
    "\"\"\")\n",
    "\n",
    "# Load recent impact records for dedup\n",
    "dedup_cutoff = now_utc - timedelta(hours=config.dedup_window_hours)\n",
    "\n",
    "try:\n",
    "    recent_impacts = spark.sql(f\"\"\"\n",
    "        SELECT CONCAT(alert_id, '||', policy_id) AS dedup_key\n",
    "        FROM   weather_impact_events\n",
    "        WHERE  created_at >= '{dedup_cutoff.isoformat()}'\n",
    "    \"\"\")\n",
    "    seen_keys = set(row[\"dedup_key\"] for row in recent_impacts.collect())\n",
    "except Exception:\n",
    "    seen_keys = set()\n",
    "\n",
    "print(f\"  Already-processed (alert, policy) pairs in last {config.dedup_window_hours}h: {len(seen_keys)}\")\n",
    "\n",
    "new_impacts = [\n",
    "    imp for imp in impacts\n",
    "    if f\"{imp['alert_id']}||{imp['policy_id']}\" not in seen_keys\n",
    "]\n",
    "\n",
    "print(f\"  New impacts after dedup: {len(new_impacts)}  (skipped {len(impacts) - len(new_impacts)})\")\n",
    "\n",
    "# Sort by risk score descending for publishing priority\n",
    "new_impacts.sort(key=lambda x: x[\"risk_score\"], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-0008",
   "metadata": {},
   "source": [
    "---\n",
    "## üì§ Step 5 ‚Äî Publish `policy.weather.impact` Events to Event Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0000009",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "if not new_impacts:\n",
    "    print(\"‚ÑπÔ∏è  No new impacts to publish.\")\n",
    "else:\n",
    "    print(f\"üì§ Publishing {len(new_impacts)} policy.weather.impact event(s) to Event Grid...\\n\")\n",
    "\n",
    "    published, failed, local_only = 0, 0, 0\n",
    "\n",
    "    for imp in new_impacts:\n",
    "        subject = f\"weather/alert/{imp['alert_id'][:40]}/policy/{imp['policy_id']}\"\n",
    "\n",
    "        # Construct a clean, serialisable event payload\n",
    "        event_data = {\n",
    "            \"impact_id\":          imp[\"impact_id\"],\n",
    "            \"alert_id\":           imp[\"alert_id\"],\n",
    "            \"policy_id\":          imp[\"policy_id\"],\n",
    "            \"alert_event\":        imp[\"alert_event\"],\n",
    "            \"alert_headline\":     imp[\"alert_headline\"],\n",
    "            \"alert_severity\":     imp[\"alert_severity\"],\n",
    "            \"alert_urgency\":      imp[\"alert_urgency\"],\n",
    "            \"alert_area_desc\":    imp[\"alert_area_desc\"],\n",
    "            \"alert_onset\":        imp[\"alert_onset\"].isoformat() if imp[\"alert_onset\"] else None,\n",
    "            \"alert_expires\":      imp[\"alert_expires\"].isoformat() if imp[\"alert_expires\"] else None,\n",
    "            \"business_name\":      imp[\"business_name\"],\n",
    "            \"business_type\":      imp[\"business_type\"],\n",
    "            \"city\":               imp[\"city\"],\n",
    "            \"state\":              imp[\"state\"],\n",
    "            \"zip_code\":           imp[\"zip_code\"],\n",
    "            \"contact_email\":      imp[\"contact_email\"],\n",
    "            \"contact_phone\":      imp[\"contact_phone\"],\n",
    "            \"distance_km\":        imp[\"distance_km\"],\n",
    "            \"risk_score\":         imp[\"risk_score\"],\n",
    "            \"threshold_minutes\":  imp[\"threshold_minutes\"],\n",
    "            \"hourly_rate\":        imp[\"hourly_rate\"],\n",
    "            \"max_payout\":         imp[\"max_payout\"],\n",
    "            \"run_id\":             imp[\"run_id\"],\n",
    "        }\n",
    "\n",
    "        if eg_client:\n",
    "            ok = eg_client.publish_event(\n",
    "                event_type = config.EVT_POLICY_WEATHER_IMPACT,\n",
    "                subject    = subject,\n",
    "                data       = event_data,\n",
    "            )\n",
    "            if ok:\n",
    "                imp[\"eventgrid_status\"] = \"published\"\n",
    "                published += 1\n",
    "                print(f\"  ‚úÖ Published: {imp['business_name']} ({imp['city']}, {imp['state']}) \"\n",
    "                      f\"‚Äî {imp['alert_event']} | risk={imp['risk_score']} | {imp['distance_km']} km\")\n",
    "            else:\n",
    "                imp[\"eventgrid_status\"] = \"failed\"\n",
    "                failed += 1\n",
    "                print(f\"  ‚ùå Failed:    {imp['business_name']} ({imp['policy_id']})\")\n",
    "        else:\n",
    "            imp[\"eventgrid_status\"] = \"local_only\"\n",
    "            local_only += 1\n",
    "            print(f\"  üìã Local:    {imp['business_name']} ({imp['city']}, {imp['state']}) \"\n",
    "                  f\"‚Äî {imp['alert_event']} | risk={imp['risk_score']}\")\n",
    "\n",
    "    print(f\"\\n  Published: {published}  |  Failed: {failed}  |  Local-only: {local_only}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-0009",
   "metadata": {},
   "source": [
    "---\n",
    "## üíæ Step 6 ‚Äî Persist to Delta + Summary Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0000010",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "if new_impacts:\n",
    "    impact_schema = StructType([\n",
    "        StructField(\"impact_id\",          StringType()),\n",
    "        StructField(\"alert_id\",           StringType()),\n",
    "        StructField(\"policy_id\",          StringType()),\n",
    "        StructField(\"run_id\",             StringType()),\n",
    "        StructField(\"alert_event\",        StringType()),\n",
    "        StructField(\"alert_headline\",     StringType()),\n",
    "        StructField(\"alert_description\",  StringType()),\n",
    "        StructField(\"alert_instruction\",  StringType()),\n",
    "        StructField(\"alert_severity\",     StringType()),\n",
    "        StructField(\"alert_urgency\",      StringType()),\n",
    "        StructField(\"alert_certainty\",    StringType()),\n",
    "        StructField(\"alert_area_desc\",    StringType()),\n",
    "        StructField(\"alert_sender\",       StringType()),\n",
    "        StructField(\"alert_onset\",        TimestampType()),\n",
    "        StructField(\"alert_expires\",      TimestampType()),\n",
    "        StructField(\"alert_centroid_lat\", DoubleType()),\n",
    "        StructField(\"alert_centroid_lon\", DoubleType()),\n",
    "        StructField(\"business_name\",      StringType()),\n",
    "        StructField(\"business_type\",      StringType()),\n",
    "        StructField(\"city\",               StringType()),\n",
    "        StructField(\"state\",              StringType()),\n",
    "        StructField(\"zip_code\",           StringType()),\n",
    "        StructField(\"contact_email\",      StringType()),\n",
    "        StructField(\"contact_phone\",      StringType()),\n",
    "        StructField(\"threshold_minutes\",  IntegerType()),\n",
    "        StructField(\"hourly_rate\",        DoubleType()),\n",
    "        StructField(\"max_payout\",         DoubleType()),\n",
    "        StructField(\"policy_lat\",         DoubleType()),\n",
    "        StructField(\"policy_lon\",         DoubleType()),\n",
    "        StructField(\"distance_km\",        DoubleType()),\n",
    "        StructField(\"risk_score\",         DoubleType()),\n",
    "        StructField(\"impact_status\",      StringType()),\n",
    "        StructField(\"eventgrid_status\",   StringType()),\n",
    "        StructField(\"created_at\",         TimestampType()),\n",
    "    ])\n",
    "\n",
    "    df = spark.createDataFrame(new_impacts, schema=impact_schema)\n",
    "    df.write.format(\"delta\").mode(\"append\").saveAsTable(\"weather_impact_events\")\n",
    "    print(f\"‚úÖ Saved {len(new_impacts)} impact records to Delta table: weather_impact_events\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è  Nothing to save ‚Äî no new impacts this run.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0000011",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(f\"RUN SUMMARY ‚Äî Run ID: {RUN_ID}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"  NOAA alerts polled (qualifying severity): {len(raw_alerts)}\")\n",
    "print(f\"  NOAA alerts with usable geometry:         {len(geo_alerts)}\")\n",
    "print(f\"  Policyholder geo-matches (raw):           {len(impacts)}\")\n",
    "print(f\"  New impacts after dedup:                  {len(new_impacts)}\")\n",
    "\n",
    "if new_impacts:\n",
    "    pub_count  = sum(1 for i in new_impacts if i[\"eventgrid_status\"] == \"published\")\n",
    "    fail_count = sum(1 for i in new_impacts if i[\"eventgrid_status\"] == \"failed\")\n",
    "    local_count= sum(1 for i in new_impacts if i[\"eventgrid_status\"] == \"local_only\")\n",
    "    print(f\"  Events published to Event Grid:           {pub_count}\")\n",
    "    print(f\"  Events failed to publish:                 {fail_count}\")\n",
    "    print(f\"  Events stored locally only:               {local_count}\")\n",
    "\n",
    "print()\n",
    "print(\"--- Impacted Policyholders (sorted by risk score) ---\")\n",
    "\n",
    "try:\n",
    "    display(spark.sql(\"\"\"\n",
    "        SELECT\n",
    "            impact_id,\n",
    "            policy_id,\n",
    "            business_name,\n",
    "            city,\n",
    "            state,\n",
    "            alert_event,\n",
    "            alert_severity,\n",
    "            distance_km,\n",
    "            risk_score,\n",
    "            eventgrid_status,\n",
    "            created_at\n",
    "        FROM weather_impact_events\n",
    "        ORDER BY created_at DESC, risk_score DESC\n",
    "        LIMIT 50\n",
    "    \"\"\"))\n",
    "except Exception as e:\n",
    "    print(f\"  Could not display table: {e}\")\n",
    "\n",
    "print()\n",
    "print(\"--- Alert Breakdown ---\")\n",
    "try:\n",
    "    display(spark.sql(\"\"\"\n",
    "        SELECT\n",
    "            alert_event,\n",
    "            alert_severity,\n",
    "            COUNT(DISTINCT alert_id)  AS distinct_alerts,\n",
    "            COUNT(DISTINCT policy_id) AS impacted_policies,\n",
    "            ROUND(AVG(distance_km), 1) AS avg_distance_km,\n",
    "            ROUND(AVG(risk_score), 3)  AS avg_risk_score\n",
    "        FROM weather_impact_events\n",
    "        WHERE run_id = '{run_id}'\n",
    "        GROUP BY alert_event, alert_severity\n",
    "        ORDER BY avg_risk_score DESC\n",
    "    \"\"\".replace(\"{run_id}\", RUN_ID)))\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "print(f\"\\n‚úÖ Notebook complete. Next step ‚Üí run weather_impact_email_notifier.ipynb\")"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "Synapse PySpark",
   "language": "Python",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "notebook_environment": {},
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "save_output": true,
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.dynamicAllocation.enabled": "false"
    },
    "enableDebugMode": false
   }
  },
  "synapse_widget": {
   "state": {},
   "version": "0.1"
  },
  "widgets": {}
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
